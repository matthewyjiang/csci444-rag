questions,answers
"What is natural language in contrast to formal programming languages?","Language used by humans that is not strictly formal or logical."
"What is one success story in NLP mentioned in the document?","Machine Translation."
"What is the primary goal of text classification?","To assign labels to text based on its content."
"What does naive bayes assume about features?","They are conditionally independent given the class label."
"What is tokenization in NLP?","The process of splitting text into smaller units like words or subwords."
"What is Zipf's law?","A law stating that word frequency is inversely proportional to rank."
"What is smoothing in NLP?","A technique to prevent zero probabilities in language models."
"What is the purpose of Laplace smoothing?","To assign a small probability to unseen events."
"What are precision and recall used for in text classification?","To evaluate the performance of a classification model."
"What is perplexity in language models?","A measure of how well a model predicts a sequence of words."
"How does a bigram model estimate probabilities?","By considering one preceding word."
"Why is log-likelihood used in NLP models?","To prevent floating-point underflow."
"What happens to data sparsity as n increases in n-gram models?","It increases."
"What is the purpose of a validation set?","To adjust hyperparameters and simulate test performance."
"What is a unigram model?","A language model that considers only single token probabilities."
"What is cross-entropy loss used for?","To penalize incorrect predictions in classification tasks."
"What does the softmax function do?","Converts logits into a probability distribution."
"How is a word embedding represented?","As a dense vector in continuous space."
"What does the bag-of-words model ignore?","Word order."
"What is a common issue with n-gram models?","Sparsity of data."
"What is the role of supervised methods in NLP?","They lay the groundwork for unsupervised methods."
"What are the components of an n-gram model?","The sequence of n tokens and their probabilities."
"What does the term 'smoothing' refer to in NLP?","Distributing probability mass to unseen events."
"What is an example of an NLP application?","Text classification, machine translation, or dialogue systems."
"What does f-measure combine?","Precision and recall."
"How does Zipf's law describe word frequency?","As an inverse relation to word rank."
"What is the harmonic mean of precision and recall called?","F-measure."
"What is a Naive Bayes classifier used for?","Predicting class labels for text based on features."
"What is the purpose of a test set in text classification?","To evaluate the final performance of the model."
"What does tokenization determine in NLP?","The granularity of text features for models."
"What does the process of stemming do?","Reduces words to their root forms."
"What does Zipf's law suggest about natural language?","Most words occur infrequently."
"What is a token in NLP?","A unit of text, such as a word or subword."
"What is the significance of smoothing in n-gram models?","It prevents probabilities from being zero for unseen data."
"What is a common use for language models?","Text generation or scoring."
"What does conditional independence mean in Naive Bayes?","Features are assumed independent given the class label."
"What is one drawback of large n in n-gram models?","Increased data sparsity and storage requirements."
"What is the goal of text-to-image systems in NLP?","To generate images based on textual descriptions."
"How do word embeddings relate words in vector space?","By encoding syntactic and semantic similarities."
"What is perplexity inversely proportional to in language models?","Model accuracy."
"What is a key assumption of Naive Bayes?","Features are conditionally independent given the class label."
"What is a unigram model's primary limitation?","It lacks context beyond single tokens."
"What do cross-entropy loss calculations rely on?","The predicted probability distribution and true labels."
"What is a potential problem with unseen words in text classification?","They may not be represented in the model."
"Why are dense word embeddings preferred?","To reduce data sparsity and capture semantic relationships."
"What is an advantage of bigram models over unigram models?","They consider limited context from preceding words."
"What is a key challenge with larger vocabularies in NLP?","Increased sparsity and computational cost."
"What are n-grams used for in NLP?","To model sequences of words and predict text."
"What does the harmonic mean emphasize in evaluation metrics?","Balancing precision and recall."
